{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "iZaM_4rPwZSX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk;\n",
        "nltk.download(\"popular\");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jamrftyb0DKn",
        "outputId": "99b4741e-0e2e-4168-8663-383c8ccc63d2"
      },
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectCorpus:\n",
        "\n",
        "  def __init__(self, corpus):\n",
        "    '''\n",
        "    Takes a corpus, tokenizes it, and sets the internal variables needed\n",
        "    to return the index of any given word (and what word has a given index)\n",
        "    Args:\n",
        "      corpus (csv): \n",
        "      The corpus is a csv,\n",
        "      with the sentenses and classes (delimiter : tab)\n",
        "    '''\n",
        "    self.data = pd.read_csv(corpus, delimiter='\\t', header=None)\n",
        "    self.data.columns = ['Sentence', 'Class']\n",
        "    self.data['index'] = data.index\n",
        "    self.columns = ['index', 'Class', 'Sentence']\n",
        "\n",
        "  def preprocess_pandas(self):\n",
        "    df_ = pd.DataFrame(columns=self.columns)\n",
        "    self.preprocessed_data = self.data.copy()\n",
        "    self.preprocessed_data ['Sentence'] = self.preprocessed_data ['Sentence'].str.lower()\n",
        "    self.preprocessed_data ['Sentence'] = self.preprocessed_data ['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
        "    self.preprocessed_data ['Sentence'] = self.preprocessed_data ['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
        "    self.preprocessed_data ['Sentence'] = self.preprocessed_data ['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
        "    self.preprocessed_data ['Sentence'] = self.preprocessed_data ['Sentence'].replace('\\d', '', regex=True)                                                                                         # remove numbers\n",
        "    for index, row in self.preprocessed_data .iterrows():\n",
        "        word_tokens = word_tokenize(row['Sentence'])\n",
        "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_ = df_.append({\n",
        "            \"index\": row['index'],\n",
        "            \"Class\": row['Class'],\n",
        "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
        "        }, ignore_index=True)\n",
        "    return self.preprocessed_data \n",
        "\n",
        "  def data_split(self, tsize, randomsize, shuffleTF):\n",
        "    if hasattr(self, 'preprocessed_data'):\n",
        "      self.training_data, self.validation_data, self.training_labels, self.validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "      self.preprocessed_data['Sentence'].values.astype('U'),\n",
        "      self.preprocessed_data['Class'].values.astype('int32'),\n",
        "      test_size=tsize,\n",
        "      random_state=randomsize,\n",
        "      shuffle=shuffleTF)\n",
        "    else:\n",
        "      self.training_data, self.validation_data, self.training_labels, self.validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "      self.data['Sentence'].values.astype('U'),\n",
        "      self.data['Class'].values.astype('int32'),\n",
        "      test_size=tsize,\n",
        "      random_state=randomsize,\n",
        "      shuffle=shuffleTF)\n",
        "\n"
      ],
      "metadata": {
        "id": "q51nr4f_Tcc1"
      },
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CorpusTraining:\n",
        "\n",
        "  def data_split(self, df, tsize, randomsize, shuffleTF):\n",
        "    if hasattr(df, 'preprocessed_data'):\n",
        "      self.training_data, self.validation_data, self.training_labels, self.validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "      df.preprocessed_data['Sentence'].values.astype('U'),\n",
        "      df.preprocessed_data['Class'].values.astype('int32'),\n",
        "      test_size=tsize,\n",
        "      random_state=randomsize,\n",
        "      shuffle=shuffleTF)\n",
        "    else:\n",
        "      self.training_data, self.validation_data, self.training_labels, self.validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "      df.data['Sentence'].values.astype('U'),\n",
        "      df.data['Class'].values.astype('int32'),\n",
        "      test_size=tsize,\n",
        "      random_state=randomsize,\n",
        "      shuffle=shuffleTF)\n",
        "    \n",
        "    self.classes = len(df.data['Class'].unique())\n",
        "\n",
        "    return None\n",
        "\n",
        "  def vectorize1(self):\n",
        "    self.word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
        "\n",
        "    self.training_vectors = self.word_vectorizer.fit_transform(self.training_data)\n",
        "    self.training_vectors = self.training_vectors.todense()\n",
        "    self.vocab_size = len(self.word_vectorizer.vocabulary_)\n",
        "    self.validation_vectors = self.word_vectorizer.transform(self.validation_data)\n",
        "    self.validation_vectors = self.validation_vectors.todense()\n",
        "    self.train_x_tensor = torch.from_numpy(np.array(self.training_vectors)).type(torch.FloatTensor)\n",
        "    self.train_y_tensor = torch.from_numpy(np.array(self.training_labels)).long()\n",
        "    self.validation_x_tensor = torch.from_numpy(np.array(self.validation_vectors)).type(torch.FloatTensor)\n",
        "    self.validation_y_tensor = torch.from_numpy(np.array(self.validation_labels)).long()\n",
        "\n",
        "\n",
        "  def vectorize2(self):\n",
        "    self.word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
        "\n",
        "    self.all_vectors = self.word_vectorizer.fit_transform(np.concatenate((self.training_data, self.validation_data)))\n",
        "    self.all_vectors = self.all_vectors.todense()\n",
        "    self.vocab_size = len(self.word_vectorizer.vocabulary_)\n",
        "\n",
        "    self.training_vectors = self.word_vectorizer.transform(self.training_data)\n",
        "    self.training_vectors = self.training_vectors.todense()\n",
        "    \n",
        "    self.validation_vectors = self.word_vectorizer.transform(self.validation_data)\n",
        "    self.validation_vectors = self.validation_vectors.todense()\n",
        "\n",
        "    # self.train_x_tensor = torch.from_numpy(np.array(self.training_vectors)).type(torch.FloatTensor)\n",
        "    # self.train_y_tensor = torch.from_numpy(np.array(self.training_labels)).long()\n",
        "\n",
        "    # self.train_dataset = [torch.from_numpy(np.array(self.training_vectors)).type(torch.FloatTensor),\n",
        "    #                       torch.from_numpy(np.array(self.training_labels)).long()\n",
        "    #                       ]\n",
        "\n",
        "    self.train_dataset = [[x, y] for x,y in zip(torch.from_numpy(np.array(self.training_vectors)).type(torch.FloatTensor),torch.from_numpy(np.array(self.training_labels)).long())]\n",
        "\n",
        "\n",
        "    # self.validation_x_tensor = torch.from_numpy(np.array(self.validation_vectors)).type(torch.FloatTensor)\n",
        "    # self.validation_y_tensor = torch.from_numpy(np.array(self.validation_labels)).long()\n",
        "\n",
        "    # self.validation_dataset = [torch.from_numpy(np.array(self.validation_vectors)).type(torch.FloatTensor),\n",
        "    #                            torch.from_numpy(np.array(self.validation_labels)).long()\n",
        "    #                            ]\n",
        "\n",
        "    self.validation_dataset = [[x, y] for x,y in zip(torch.from_numpy(np.array(self.validation_vectors)).type(torch.FloatTensor),torch.from_numpy(np.array(self.validation_labels)).long())]\n",
        "\n",
        "  def token2index(self, token):\n",
        "    return self.word_vectorizer.vocabulary_[token]\n",
        "\n",
        "\n",
        "  def index2token(self, idx):\n",
        "    return list(self.word_vectorizer.vocabulary_.keys())[list(self.word_vectorizer.vocabulary_.values()).index(idx)]\n",
        "\n",
        "  def nn_sequential(self,hidden, lrate):\n",
        "    self.network = nn.Sequential( \n",
        "    nn.Linear(self.vocab_size, hidden),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden, self.classes)\n",
        "    )\n",
        "\n",
        "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr = lrate)\n",
        "    self.loss_function = nn.CrossEntropyLoss() \n",
        "\n",
        "  def train(self, epochs, batch_size):\n",
        "    for epoch in range(epochs):\n",
        "      for batch_nr, (data, labels) in enumerate(DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True)):\n",
        "        print(batch_nr)\n"
      ],
      "metadata": {
        "id": "Y2Pa3cR_pVzV"
      },
      "execution_count": 377,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = ProjectCorpus(\"amazon_cells_labelled.txt\")"
      ],
      "metadata": {
        "id": "-pwoLq35WVkq"
      },
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.preprocess_pandas();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUzxOA68Wmrp",
        "outputId": "0dc7ff8f-fd10-4ac7-9a52-6693630ecd6c"
      },
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-375-09d801de6875>:23: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  self.preprocessed_data ['Sentence'] = self.preprocessed_data ['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mytrainer = CorpusTraining()\n",
        "mytrainer.data_split(df, 0.10, 0, True)"
      ],
      "metadata": {
        "id": "29XZ-XZHp3gm"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytrainer.vectorize2()"
      ],
      "metadata": {
        "id": "Kr1cGNwExqAA"
      },
      "execution_count": 387,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytrainer.nn_sequential(100, 0.02)"
      ],
      "metadata": {
        "id": "b401y08wA6rH"
      },
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mytrainer.train(10,100) # (epochs, batch size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qUkieBmLRZ0",
        "outputId": "c24cf0cc-407e-4cd4-ac17-6555afec42b1"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TEST AREA**"
      ],
      "metadata": {
        "id": "0b6mE1qfCyqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_nr, (data, labels) in enumerate(DataLoader(mytrainer.train_dataset, batch_size=10, shuffle=True)):\n",
        "  print(batch_nr)\n",
        "  print(data)\n",
        "  print(labels)\n",
        "  print(\"------\")\n",
        "  if batch_nr>2:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqQVxU0IK1R3",
        "outputId": "bcb888fe-87f8-4a39-f0ab-1cc8dd5077f8"
      },
      "execution_count": 391,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0])\n",
            "------\n",
            "1\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0])\n",
            "------\n",
            "2\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 0])\n",
            "------\n",
            "3\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0])\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mytrainer.token2index('sister')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiYxdkLpxCGj",
        "outputId": "f031712e-67bc-48b5-edb0-bd977c7a8372"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5816"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mytrainer.index2token(5816)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mALfDwcU16Ea",
        "outputId": "f24a04af-4584-40e1-a68e-d6f2793c522e"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sister'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    }
  ]
}